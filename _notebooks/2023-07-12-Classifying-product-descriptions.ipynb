{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying products according to UNSPSC using their descriptions\n",
    "[UNSPSC](https://en.wikipedia.org/wiki/UNSPSC) is a taxonomy for product classification. Companies can use it to categorize their products and services for internal use, accounting, and reporting.\n",
    "The UNSPSC is a four-level hierarchy with the following levels:\n",
    "1. Segment\n",
    "2. Family\n",
    "3. Class\n",
    "4. Commodity\n",
    "\n",
    "**Hypothetical example:**\n",
    "1. Segment: Food and Beverage Products and Services\n",
    "2. Family: Fresh food products\n",
    "3. Class: Fresh fruits\n",
    "4. Commodity: Fresh apples\n",
    "\n",
    "## The problem\n",
    "Given a product description, we want to categorize it according to the UNSPSC taxonomy.\n",
    "A notebook trying different approaches to categorizing product descriptions.\n",
    "\n",
    "## Load UNSPSC codes\n",
    "Our dataset consists of `segment`, `family`, `class`, `commodity`. Each of these is a table of prefixes and descriptions. Where segment only specifies the first two digits of the code, family specifies the first four, class the first six and commodity the full eight digits. \n",
    "\n",
    "A sample of the data:\n",
    "- 43: IT and network and telecommunications - **Segment**\n",
    "- 4322: Equipment or platforms for voice/data or multimedia networks, as well as accessories - **Family**\n",
    "- 432225: Network security equipment - **Class**\n",
    "- 43222501: Network firewalls - **Commodity**\n",
    "\n",
    "Each UNSPSC code has a description in Swedish. For example `43222501` is the code for `Networking firewalls`.\n",
    "\n",
    "Now let's load in the datasets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of segments:  55\n",
      "Length of families:  388\n",
      "Length of classes:  3114\n",
      "Length of commodities:  38104\n"
     ]
    }
   ],
   "source": [
    "from descent import segments, families, classes, commodities\n",
    "print(\"Length of segments: \", len(segments))\n",
    "print(\"Length of families: \", len(families))\n",
    "print(\"Length of classes: \", len(classes))\n",
    "print(\"Length of commodities: \", len(commodities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating embeddings for Swedish text\n",
    "The [KB](https://kb.se) has published a number of pretained tranformers that are free to download and use.\n",
    "For this application, we are going to use a sentence embedding variant called `KBLab/sentence_bert_swedish`.\n",
    "\n",
    "First, we need to load the model and tokenizer. We are going to use the `AutoTokenizer` and `AutoModel` classes from the `transformers` library.\n",
    "Then we define a function `embed` that takes a list of strings and returns a list of embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolo/dev/neodev-llm/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "# To load an older version, e.g. v1.0, add the argument revision=\"v1.0\" \n",
    "tokenizer = AutoTokenizer.from_pretrained('KBLab/sentence-bert-swedish-cased')\n",
    "model = AutoModel.from_pretrained('KBLab/sentence-bert-swedish-cased')\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    \"\"\"\n",
    "    Mean pooling of the token embeddings, weighted by the attention mask.\n",
    "    \"\"\"\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def embed(texts: str):\n",
    "    \"\"\"\n",
    "    Embeds a list of texts using the Sentence-BERT model.\n",
    "    \"\"\"\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Perform pooling. In this case, max pooling.\n",
    "    embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check\n",
    "Let's try to embed a couple of sentences and see what we get.\n",
    "The sentences translate to:\n",
    "- I am walking in the woods\n",
    "- I am walking in the desert\n",
    "- I am swiming in the ocean\n",
    "- I am bathing in a lake\n",
    "\n",
    "We expect that walking in the woods and walking in the desert are more similar than swimming in the ocean and bathing in a lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Jag går i skogen\"\t is most similar to \t\"Jag går på stan\"\n",
      "\n",
      "\"Jag går på stan\"\t is most similar to \t\"Jag går i skogen\"\n",
      "\n",
      "\"Jag simmar i havet\"\t is most similar to \t\"Jag badar i sjön\"\n",
      "\n",
      "\"Jag badar i sjön\"\t is most similar to \t\"Jag simmar i havet\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sentences we want sentence embeddings for\n",
    "sentence1 = \"Jag går i skogen\"\n",
    "sentence2 = \"Jag går på stan\"\n",
    "sentence3 = \"Jag simmar i havet\"\n",
    "sentence4 = \"Jag badar i sjön\"\n",
    "\n",
    "sentences = [sentence1, sentence2, sentence3, sentence4]\n",
    "\n",
    "# Compute cosine-similarities for each sentence with each other sentence\n",
    "embeddings = embed(sentences)\n",
    "for i in range(len(embeddings)):\n",
    "    most_similar_index = 0\n",
    "    most_similar_score = 0\n",
    "    for j in range(len(embeddings)):\n",
    "        if i == j: continue\n",
    "        score = 1-cosine(embeddings[i], embeddings[j])\n",
    "        if score > most_similar_score:\n",
    "            most_similar_score = score\n",
    "            most_similar_index = j\n",
    "    print(f\"\\\"{sentences[i]}\\\"\\t is most similar to \\t\\\"{sentences[most_similar_index]}\\\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_item(d: dict):\n",
    "    return list(d.items())[0]\n",
    "\n",
    "def filter_by_prefix(prefixes, categories: dict):\n",
    "    \"\"\"\n",
    "    Takes a prefix or a list of prefixes and a dictionary of\n",
    "    prefixes->descriptions and returnes a filtered dictionary where only the\n",
    "    prefixes that start with the given prefixes are kept.\n",
    "    \"\"\"\n",
    "    if type(prefixes) == str:\n",
    "        return {k: v for k, v in categories.items() if k.startswith(prefixes)}\n",
    "    elif type(prefixes) == list:\n",
    "        return {k: v for k, v in categories.items() if any(k.startswith(prefix) for prefix in prefixes)}\n",
    "\n",
    "        \n",
    "\n",
    "def most_similar(text: str, prefixes: dict, n = 1)->dict:\n",
    "    \"\"\"\n",
    "    Takes a text and a dictionary of prefixes->descriptions and returns a\n",
    "    dictionary of the n most similar prefixes and their descriptions.\n",
    "\n",
    "    It performes the following steps:\n",
    "    1. Embeds the text and the descriptions\n",
    "    2. Computes the cosine similarity between the text and the descriptions\n",
    "    3. Returns the n most similar prefixes and their descriptions\n",
    "    \"\"\"\n",
    "    embeddings = embed([text] + list(prefixes.values()))\n",
    "    text_embedding = embeddings[0]\n",
    "\n",
    "    scores = {} # prefix -> score\n",
    "    for i in range(1, len(embeddings)):\n",
    "        prefix = list(prefixes.keys())[i-1]\n",
    "        scores[prefix] = 1-cosine(text_embedding, embeddings[i])\n",
    "    \n",
    "    top_scores = dict(sorted(scores.items(), key=lambda item: item[1], reverse=True)[:n])\n",
    "    \n",
    "    # Filter the prefixes and only keep the ones from top_scores\n",
    "    return {k: v for k, v in prefixes.items() if k in top_scores}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Approaches\n",
    "\n",
    "### Super naive approach\n",
    "Just just go through all of them and find the most similar one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Äpple Golden Delicious EU x13kg/kg\"\n",
    "\n",
    "top_results = most_similar(text, list(commodities.values()), 5)\n",
    "\n",
    "for prefix, description in top_results.items():\n",
    "    print(prefix, description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The super naive approach didn't work. After 2 minutes on my MacBook Air M1, I had to interrupt the kernel. Let's try something else...\n",
    "\n",
    "### Descent\n",
    "Comparing too many descriptions at a time is too slow. Luckily, the UNSPSC codes are hierarchical. This means that we can start at the top and then only consider the descriptions that are children of the previous level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Äpple Golden Delicious EU x13kg/kg\n",
      "50 Mat, dryck och tobaksprodukter\n",
      "5036 Fruktkonserver\n",
      "503615 Konserverade eller inlagda äpplen\n",
      "50361588 Konserverade eller inlagda äpplen, Twenty ounce\n"
     ]
    }
   ],
   "source": [
    "text = \"Äpple Golden Delicious EU x13kg/kg\"\n",
    "\n",
    "print(text)\n",
    "similar_segment = most_similar(text, segments)\n",
    "segment_prefix, segment_name= get_first_item(similar_segment)\n",
    "print(segment_prefix, segment_name)\n",
    "\n",
    "filtered_families = filter_by_prefix(segment_prefix, families)\n",
    "similar_family = most_similar(text, filtered_families)\n",
    "families_prefix, families_name = get_first_item(similar_family)\n",
    "print(families_prefix, families_name)\n",
    "\n",
    "filtered_classes = filter_by_prefix(families_prefix, classes)\n",
    "similar_class = most_similar(text, filtered_classes)\n",
    "classes_prefix, classes_name = get_first_item(similar_class)\n",
    "print(classes_prefix, classes_name)\n",
    "\n",
    "filtered_commodities = filter_by_prefix(classes_prefix, commodities)\n",
    "similar_commodity = most_similar(text, filtered_commodities)\n",
    "commodities_prefix, commodities_name = get_first_item(similar_commodity)\n",
    "print(commodities_prefix, commodities_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not great, but it's a start. No idea why it decided that the apples are \"Konserverade eller inlagda\" (preserved) and not \"Färsk frukt\" (fresh fruit).\n",
    "\n",
    "What if it dives too quickly into the wrong branch? Let's try to only consider the top `n` most similar descriptions.\n",
    "\n",
    "### Checking the top results for every layer\n",
    "Let's preserve the top `n` most similar descriptions for each level and then try to find the best path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KALKON RÖKT SK BRICKA 1KG M/\n",
      "\n",
      "11 Mineraler och textilier samt oätbara växt- och djurdelar\n",
      "15 Bränslen och bränsletillsatser och smörjmedel och korrosionshindrande medel\n",
      "50 Mat, dryck och tobaksprodukter\n",
      "\n",
      "1118 Metalloxid\n",
      "1510 Bränslen\n",
      "5038 Fruktpuré\n",
      "\n",
      "151016 Bränslen i fast form och gelform\n",
      "151017 Bränsleoljor\n",
      "503818 Puréer av meloner\n",
      "\n",
      "15101602 Brunkol\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text, end=\"\\n\\n\")\n",
    "\n",
    "N = 3\n",
    "\n",
    "similar_segments = most_similar(text, segments, n=N)\n",
    "segment_prefixes = list(similar_segments.keys())\n",
    "for prefix, description in similar_segments.items():\n",
    "    print(prefix, description)\n",
    "print()\n",
    "\n",
    "filtered_families = filter_by_prefix(segment_prefixes, families)\n",
    "similar_families = most_similar(text, filtered_families, n=N)\n",
    "for prefix, description in similar_families.items():\n",
    "    print(prefix, description)\n",
    "print()\n",
    "\n",
    "filtered_classes = filter_by_prefix(list(similar_families.keys()), classes)\n",
    "similar_classes = most_similar(text, filtered_classes, n=N)\n",
    "for prefix, description in similar_classes.items():\n",
    "    print(prefix, description)\n",
    "print()\n",
    "\n",
    "filtered_commodities = filter_by_prefix(list(similar_classes.keys()), commodities)\n",
    "similar_commodities = most_similar(text, filtered_commodities, n=1)\n",
    "for prefix, description in similar_commodities.items():\n",
    "    print(prefix, description)\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This worked very well for the apples (not shown), but for the smoked turkey (Rökt kalkon) it got tricked by the word \"rökt\" (smoked). It thought that it was something combustable. We need some better understanding of the text.\n",
    "\n",
    "## Language Models\n",
    "Now, let's try to use generative language models to see if they can do better.\n",
    "\n",
    "`languagemodels` is a python library that makes it easy to use language models for a variety of tasks. It's built on top of the `transformers` library. \n",
    "\n",
    "Lets see how it does!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import languagemodels as lm\n",
    "lm.set_max_ram('4gb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the prompt be a question and a number of options. The model's job is to pick the correct option. Here is an example prompt:\n",
    "\n",
    "```\n",
    "Which of the following is 'Äpple Golden Delicious EU x13kg/kg' most similar to? Only answer with the number within the parenthesis.\n",
    "(0) option 1\n",
    "(1) option 2\n",
    "...\n",
    "(9) option 10\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ⁇ pple Golden Delicious EU x13kg/kg\n"
     ]
    }
   ],
   "source": [
    "text = \"Äpple Golden Delicious EU x13kg/kg\"\n",
    "prompt = f\"Vilken av följande är '{text}' mest lik? Svara endast med siffran inom paranteserna\\n\"\n",
    "for i, option in enumerate(list(segments.values())):\n",
    "    prompt += f\"({i}) {option}\\n\"\n",
    "\n",
    "print(lm.do(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great... Also notice how it also took 8 seconds for just the first level? Even if that would have produced a good answer it took too long to be useful.\n",
    "\n",
    "Let's try to use the classifier instead.\n",
    "\n",
    "### Tournament approach\n",
    "As we only can classify between two labels at a time, we need to do a lot of comparisons. Let's try to do it in a more efficient way.\n",
    "This works in the same way as a football turnament. We start with `n` teams and then we have `n/2` matches. The winners of those matches then play against each other and so on until we have a winner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_list(lst, n):\n",
    "    return [lst[i:i+n] for i in range(0, len(lst), n)]\n",
    "\n",
    "def elimination(text: str, descriptions: list):\n",
    "    \"\"\"\n",
    "    Takes a text and a list of descriptions and returns the most similar description's index through elimination\n",
    "    \"\"\"\n",
    "    if len(descriptions) == 1:\n",
    "        return descriptions[0]\n",
    "\n",
    "    pairs = chunk_list(descriptions, 2)\n",
    "    \n",
    "    # Compute winner of each pair\n",
    "    winners = []\n",
    "    for pair in pairs:\n",
    "        if len(pair) == 1:\n",
    "            winners.append(pair[0])\n",
    "        else:\n",
    "            winners.append(lm.classify(text, pair[0], pair[1]))\n",
    "\n",
    "    return elimination(text, winners)\n",
    "\n",
    "\n",
    "text = \"Äpple Golden Delicious EU x13kg/kg\"\n",
    "print(elimination(text, list(segments.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was also too slow! I do not think that this library is going to work out...\n",
    "\n",
    "*I later realized that this model was probably only trained on English data. No wonder it performs so poorly on Swedish text.*\n",
    "\n",
    "### Large Language Model (LLM) using Open AI's API\n",
    "Let's bring in the big guns! Open AI has released a number of language models that can be used through their [API](https://openai.com/blog/openai-api).\n",
    "\n",
    "Interestingly enough, the way we interface with the model is through a conversation between a `user` and a `assistant`.\n",
    "\n",
    "**The API basically works like this:**\n",
    "1. You give the model a system prompt - this is the context that the model will use to generate the response.\n",
    "2. You give the model a user prompt - this is what the user would say to the assistant.\n",
    "3. The model generates a response.\n",
    "\n",
    "**How we are going to use it:**\n",
    "1. Tell the model that it is a product classifier.\n",
    "2. Give the model a prompt. The same as shown earlier.\n",
    "3. We then extract the model's choice using regex and use that to determine which branch to go down in the classification tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KALKON RÖKT SK BRICKA 1KG M/\n",
      "\n",
      "50 Mat, dryck och tobaksprodukter\n",
      "5011 Kött- och fjäderfäprodukter\n",
      "501120 Bearbetade kött- och fjäderfäprodukter\n",
      "50112012 Kalkon, förädlat utan tillsatser\n",
      "Final answer: 50112012\n",
      "Total tokens: 2639\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import re\n",
    "\n",
    "with open(\"../api-key.txt\") as f:\n",
    "    openai.api_key = f.read().strip()\n",
    "\n",
    "total_tokens = 0\n",
    "\n",
    "def generate_prompt(text: str, descriptions: list):\n",
    "    \"\"\"\n",
    "    Takes a text and a list of descriptions and returns a prompt for OpenAI's API\n",
    "    \"\"\"\n",
    "    prompt = f\"Vilken av följande är '{text}'? Svara ENDAST med numret inom paranteserna.\\n\"\n",
    "    for i, option in enumerate(descriptions):\n",
    "        prompt += f\"({i}) {option}\\n\"\n",
    "    return prompt\n",
    "\n",
    "def guess(text, descriptions):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Du är en produktklassificerare. Hjälp till att klassificera produkter genom att svara på frågor.\"},\n",
    "        {\"role\":\"user\", \"content\": generate_prompt(text, list(descriptions.values()))}\n",
    "    ]\n",
    "    completion = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=messages,\n",
    "                temperature=0.0,\n",
    "            )\n",
    "    \n",
    "    global total_tokens\n",
    "    total_tokens += int(completion[\"usage\"][\"total_tokens\"])\n",
    "\n",
    "    response = completion[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    # Extract the model's guess using regex\n",
    "    number = int(re.search(r\"\\d+\", response).group(0))\n",
    "\n",
    "    description = list(descriptions.values())[int(number)]\n",
    "    prefix = list(descriptions.keys())[int(number)]\n",
    "\n",
    "    return prefix, description\n",
    "\n",
    "text = \"KALKON RÖKT SK BRICKA 1KG M/\"\n",
    "\n",
    "print(text, end=\"\\n\\n\")\n",
    "\n",
    "segment_prefix, segment_name = guess(text, segments)\n",
    "print(segment_prefix, segment_name)\n",
    "\n",
    "filtered_families = filter_by_prefix(segment_prefix, families)\n",
    "families_prefix, families_name = guess(text, filtered_families)\n",
    "print(families_prefix, families_name)\n",
    "\n",
    "filtered_classes = filter_by_prefix(families_prefix, classes)\n",
    "classes_prefix, classes_name = guess(text, filtered_classes)\n",
    "print(classes_prefix, classes_name)\n",
    "\n",
    "filtered_commodities = filter_by_prefix(classes_prefix, commodities)\n",
    "commodities_prefix, commodities_name = guess(text, filtered_commodities)\n",
    "print(commodities_prefix, commodities_name)\n",
    "\n",
    "print(\"Final answer:\", commodities_prefix)\n",
    "print(\"Total tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was fast and quite accurate. Unfortunately, that is quite expensive. 2500 tokens costs about $0.004. It might not sound like much, but if we want to classify 1000 products, that would cost $4.\n",
    "\n",
    "### LLM + embedding approach\n",
    "What if we only used the LLM to clean up the text and then used the embedding approach to find the most similar description?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KALKON RÖKT SK BRICKA 1KG M/\n",
      "\"Rökt skivad kalkon på bricka, 1 kg\"\n",
      "Total tokens: 61\n"
     ]
    }
   ],
   "source": [
    "total_tokens = 0\n",
    "def clean_text(text: str):\n",
    "    \"\"\"\n",
    "    Takes a text and returns a human-readable version of it.\n",
    "    \"\"\"\n",
    "    prompt = \"Här kommer en fakturarad, kan du göra den lättare att förstå?\\n\"\n",
    "    prompt += \"\\\"\" + text + \"\\\"\\n\"\n",
    "\n",
    "    messages = [{\"role\":\"user\", \"content\": prompt}]\n",
    "    completion = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=messages,\n",
    "                temperature=0.0,\n",
    "            )\n",
    "    \n",
    "    global total_tokens\n",
    "    total_tokens += int(completion[\"usage\"][\"total_tokens\"])\n",
    "\n",
    "    # get the number from the response\n",
    "    response = completion[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return response\n",
    "\n",
    "text = \"KALKON RÖKT SK BRICKA 1KG M/\"\n",
    "print(text)\n",
    "cleaned_text = clean_text(text)\n",
    "print(cleaned_text)\n",
    "print(\"Total tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much clearer!\n",
    "Now maybe the embedding approach is good enough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Levande växter och djur samt tillbehör och materiel\n",
      "14 Pappersmaterial och pappersprodukter\n",
      "50 Mat, dryck och tobaksprodukter\n",
      "\n",
      "5011 Kött- och fjäderfäprodukter\n",
      "5017 Kryddor och konserver\n",
      "5019 Beredd och konserverad mat\n",
      "\n",
      "501115 Minimalt bearbetade kött- och fjäderfäprodukter\n",
      "501120 Bearbetade kött- och fjäderfäprodukter\n",
      "501927 Förpackade måltidskombinationer\n",
      "\n",
      "50112013 Kalkon, förädlat med tillsatser\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N = 3\n",
    "text = cleaned_text\n",
    "similar_segments = most_similar(text, segments, n=N)\n",
    "segment_prefixes = list(similar_segments.keys())\n",
    "for prefix, description in similar_segments.items():\n",
    "    print(prefix, description)\n",
    "print()\n",
    "\n",
    "filtered_families = filter_by_prefix(segment_prefixes, families)\n",
    "similar_families = most_similar(text, filtered_families, n=N)\n",
    "for prefix, description in similar_families.items():\n",
    "    print(prefix, description)\n",
    "print()\n",
    "\n",
    "filtered_classes = filter_by_prefix(list(similar_families.keys()), classes)\n",
    "similar_classes = most_similar(text, filtered_classes, n=N)\n",
    "for prefix, description in similar_classes.items():\n",
    "    print(prefix, description)\n",
    "print()\n",
    "\n",
    "filtered_commodities = filter_by_prefix(list(similar_classes.keys()), commodities)\n",
    "similar_commodities = most_similar(text, filtered_commodities, n=1)\n",
    "for prefix, description in similar_commodities.items():\n",
    "    print(prefix, description)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am stunned by how well this works. It is very fast and very accurate. It is also very cheap as we only used 60 tokens. Now classifying 1000 products would only cost $0.06.\n",
    "\n",
    "## Conclusion\n",
    "### Embedding approach\n",
    "This approach is very fast, but not very accurate. Sometimes it would fixate on a specific word like \"smoked\" and start guessing combustible materials.\n",
    "\n",
    "### Language model approach\n",
    "The small language model approach was very slow and not very accurate. This was probably because it was trained on English data and not Swedish. There is a [GPT trained on Swedish data](https://www.ai.se/en/node/81535/gpt-sw3) but it is not generally available at the moment. Even if it was, it would probably be too slow to be useful.\n",
    "\n",
    "Only using a large language model (Open AI) approach was reasonably fast and very accurate, but expensive. It would probably be too expensive to use in production.\n",
    "\n",
    "### LLM + embedding approach\n",
    "Cleaning up the text before using the embedding approach improved the accuracy a lot. It was also fast and cheap. This is probably the best approach for this application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
